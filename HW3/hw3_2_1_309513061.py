# -*- coding: utf-8 -*-
"""hw3_2_1_309513061.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15Z0IGSmDtQ2oPlogoSx8CH6db1Ijj6C9
"""

import numpy as np
# import finite_MDP_env as env

# e = env.environment()
prob_table = {0: {0: 0.2, 1: 0.7, 2: 0.3, 4: 1, 5: 0.2},
 1: {0: 0.9, 1: 0.2, 2: 0.7, 4: 1, 5: 0.8}}
trans_table = {0: [1, 2], 1: [3, 4], 2: [4, 5], 4: [6], 5: [6, 7]}
reward_table = {0: [15, 7], 1: [7, -15], 2: [-20, 5], 4: [30], 5: [0, 10]}
done_list = [3,6,7]
policy1 = [0.5,0.5,0.5,0,0.5,0.5,0,0]
policy2 = [0,0,1,0,0,1,0,0]

def q_table(policy,prob,trans,reward,done,dis=0.9):
  state = np.ones([8,2])
  for j in done:
    state[j][0],state[j][1] = 0,0
    
  for i in range(7,-1,-1):
    if (state[i][0] == 0.):
      continue
    elif len(trans[i]) == 1:
      state[i,0] = prob[0][i]*(reward[i][0]+dis*(policy[trans[i][0]]*state[trans[i][0],0]+(1-policy[trans[i][0]])*state[trans[i][0],1]))+\
      (1-prob[0][i])*(reward[i][0]+dis*(policy[trans[i][0]]*state[trans[i][0],0]+(1-policy[trans[i][0]])*state[trans[i][0],1]))
      state[i,1] = prob[1][i]*(reward[i][0]+dis*(policy[trans[i][0]]*state[trans[i][0],0]+(1-policy[trans[i][0]])*state[trans[i][0],1]))+\
      (1-prob[1][i])*(reward[i][0]+dis*(policy[trans[i][0]]*state[trans[i][0],0]+(1-policy[trans[i][0]])*state[trans[i][0],1]))
    else:
      state[i,0] = prob[0][i]*(reward[i][0]+dis*(policy[trans[i][0]]*state[trans[i][0],0]+(1-policy[trans[i][0]])*state[trans[i][0],1]))+\
      (1-prob[0][i])*(reward[i][1]+dis*(policy[trans[i][1]]*state[trans[i][1],0]+(1-policy[trans[i][1]])*state[trans[i][1],1]))
      state[i,1] = prob[1][i]*(reward[i][0]+dis*(policy[trans[i][0]]*state[trans[i][0],0]+(1-policy[trans[i][0]])*state[trans[i][0],1]))+\
      (1-prob[1][i])*(reward[i][1]+dis*(policy[trans[i][1]]*state[trans[i][1],0]+(1-policy[trans[i][1]])*state[trans[i][1],1]))
  return state

def v_table(policy,prob,trans,reward,done,dis=0.9):
  state = np.ones(8)
  for j in done:
    state[j] = 0

  for i in range(7,-1,-1):

    if state[i] == 0.:
      continue
    elif len(trans[i]) == 1:
      state[i] =  policy[i]*prob[0][i]*(reward[i][0]+dis*state[trans[i][0]])+policy[i]*(1-prob[0][i])*(reward[i][0]+dis*state[trans[i][0]])+\
     (1-policy[i])*prob[1][i]*(reward[i][0]+dis*state[trans[i][0]])+(1-policy[i])*(1-prob[1][i])*(reward[i][0]+dis*state[trans[i][0]])
    else:
     state[i] =  policy[i]*prob[0][i]*(reward[i][0]+dis*state[trans[i][0]])+policy[i]*(1-prob[0][i])*(reward[i][1]+dis*state[trans[i][1]])+\
     (1-policy[i])*prob[1][i]*(reward[i][0]+dis*state[trans[i][0]])+(1-policy[i])*(1-prob[1][i])*(reward[i][1]+dis*state[trans[i][1]])
  return state
      
def table_cat(p):
  table = np.zeros([8,3]) 
  table[:,0] = v_table(p,prob_table,trans_table,reward_table,done_list,0.9)
  table[:,1:] = q_table(p,prob_table,trans_table,reward_table,done_list,0.9)
  return table

t1 = table_cat(policy1)
t2 = table_cat(policy2)

np.save('value_a.npy',t1)
np.save('value_b.npy',t2)

